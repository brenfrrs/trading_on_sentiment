{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "from nltk.probability import FreqDist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('yearly_articles/apple2020.csv', index_col=0)\n",
    "df2 = pd.read_csv('yearly_articles/apple2019.csv', index_col=0)\n",
    "df3 = pd.read_csv('yearly_articles/apple2018.csv', index_col=0)\n",
    "df4 = pd.read_csv('yearly_articles/apple2017.csv', index_col=0)\n",
    "df5 = pd.read_csv('yearly_articles/apple2016.csv', index_col=0)\n",
    "df6 = pd.read_csv('yearly_articles/apple2015.csv', index_col=0)\n",
    "df = pd.concat([df1,df2, df3, df4, df5, df6])\n",
    "df.dropna(subset=['fulltext'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean newlines and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text.replace(\"\\n\",\" \")\n",
    "    text =  ' '.join(re.sub(\"([^0-9A-Za-z])\",\" \",text).split())\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['cleaned_text'] = df['fulltext'].apply(clean_text)\n",
    "df['cleaned_authors'] = df['author'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the UTC time to EST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to Datetime\n",
    "df[\"date\"]= pd.to_datetime(df[\"date\"])\n",
    "df = df.set_index('date')\n",
    "df.index = df.index.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#convert DateTime index to eastern time. \n",
    "eastern = pytz.timezone('US/Eastern')\n",
    "df.index = df.index.tz_convert(eastern).tz_localize(None)\n",
    "#put into year/month/day format\n",
    "df.index = df.index.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using [tldextract](https://pypi.org/project/tldextract/) to extract company names from url's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlet(link):\n",
    "    res = tldextract.extract(link)\n",
    "    return res.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['news_outlet'] = df['source'].apply(get_outlet)\n",
    "print('The dataset contains {} different articles from {} news outlets \\n'.format(df.shape[0],df.news_outlet.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting historical Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twelvedata import TDClient\n",
    "td = TDClient(apikey=\"ef26202dacaf412fb157a05403f81ca3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_day_range(year):\n",
    "    ranges = []\n",
    "    year = [(pd.datetime(year,1,1)), (pd.datetime(year,2,1)), (pd.datetime(year,3,1)),\n",
    "           (pd.datetime(year,4,1)), (pd.datetime(year,5,1)), (pd.datetime(year,6,1)),\n",
    "           (pd.datetime(year,7,1)), (pd.datetime(year,8,1)), (pd.datetime(year,9,1)),\n",
    "           (pd.datetime(year,10,1)), (pd.datetime(year,11,1)), (pd.datetime(year,12,1))]\n",
    "    for date in year:\n",
    "        last_day = date + relativedelta(day=1, months=+1, days=-1)\n",
    "        first_day = date + relativedelta(day=1)\n",
    "        ranges.append((first_day.strftime('%Y-%m-%d'), last_day.strftime('%Y-%m-%d')))\n",
    "    return ranges\n",
    "\n",
    "months_2020 = get_month_day_range(2020)\n",
    "months_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_2018 = get_month_day_range(2010)\n",
    "months_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather historical data from twelvedata API\n",
    "counter =1\n",
    "for start,end in months_2018:\n",
    "    time.sleep(25)\n",
    "    ts = td.time_series(\n",
    "    symbol=\"AAPL\",\n",
    "    interval=\"1day\",\n",
    "    start_date=start,\n",
    "    end_date=end\n",
    "    ).as_pandas()\n",
    "    times.append(ts)\n",
    "    print(counter, start,end)\n",
    "    counter +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices_appl = pd.concat(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the datetime indexes\n",
    "stock_prices_appl\n",
    "stock_prices_appl.index = stock_prices_appl.index.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge prices and main df's on the date\n",
    "merge=pd.merge(df,stock_prices_appl, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#localize the price df\n",
    "stock_prices_appl.index = stock_prices_appl.index.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices_appl['day_change'] = np.nan\n",
    "stock_prices_appl['increase'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_index = stock_prices_appl.index.strftime('%Y-%m-%d').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(columns = ['day_change', 'increase', 'date'])\n",
    "for i,stock_price in enumerate(prices_index):\n",
    "    try:\n",
    "        start = stock_prices_appl.loc[prices_index[i]].open[0]\n",
    "        stop = stock_prices_appl.loc[prices_index[i+1]].open[0]\n",
    "        direction = start - stop\n",
    "        if direction < 0:\n",
    "            increase = 0\n",
    "        else:\n",
    "            increase = 1\n",
    "        df_res = df_res.append({'day_change': direction, 'increase':increase, 'date':stock_price}, ignore_index=True)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[\"date\"]= pd.to_datetime(df_res[\"date\"])\n",
    "df_res = df_res.set_index('date')\n",
    "df_res.index = df_res.index.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets2020 = df_res.shift(periods=1, fill_value=0)\n",
    "targets2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets2020.to_csv('yearly_targets/targets2010.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Sentiment for each Article with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(article):\n",
    "    '''\n",
    "    VADER Sentiment used to tag the.\n",
    "    Returns the predicted labels: positive/negative/neutral.\n",
    "    Instantiate analyzer before running this function:\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    '''\n",
    "    global analyzer\n",
    "\n",
    "    score = analyzer.polarity_scores(article)\n",
    "\n",
    "    if score['compound'] >= .05:\n",
    "        sent = 'positive'\n",
    "    elif score['compound'] <= -.05:\n",
    "        sent = 'negative'\n",
    "    else:\n",
    "        sent = 'neutral'\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['sentiment'] = df['fulltext'].apply(sentiment_analyzer_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df.sentiment);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,art in enumerate(df.cleaned_text[:200]):\n",
    "#     score = sentiment_analyzer_scores(art)\n",
    "#     print(i, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize \n",
    "\n",
    "def toke(text):\n",
    "    tokens = regexp_tokenize(text, \"[\\w']+\")\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['cleaned_text'].apply(toke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlist(x):\n",
    "    return \", \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop_words=list(set(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_stopwords = [\n",
    "    'x', 'u', \"'\", 'e', 'a', 'i', 'n', 'u', 'd', 'c', 'p', 's', 'i',\n",
    "    'o', 'r', 't', 'journalism', 'support', 'u', 'editor', 'fair', 'informed',\n",
    "    'cookie', 'miamiaccording', 'article', 'expired', 'no', 'longer', 'want',\n",
    "    'search', 'google', 'every', 'term', 'newswire', 'subscribe', 'button', 'close',\n",
    "    'accept', 'goal', 'achieve', 'u', 'subscribed', 'many', 'continue', 'offer',\n",
    "    'hard', 'provide', 'dear', 'reader', 'standard', 'always', 'strived', 'miamiinterested',\n",
    "    'adopting', 'pet', 'gazing', 'lovable', 'pup', 'adoption', 'dog', 'animal', 'shelter',\n",
    "    'ziprecruiter', 'miami', 'policy', 'clicking', 'explicit', 'consent',\n",
    "    'please', 'see', 'even', 'better', 'relevant', 'goal', 'le', 'u,', 'philip', 'schiller',\n",
    "    'believe', 'getty', 'josh', 'edelson', 'topical', 'issue', 'relevance',\n",
    "    'seen', 'man', 'forward', 'dunkin', 'late', 'wife', 'bagelsee', 'rental', 'site', 'zumper',\n",
    "    'quarantinefind', 'irvine', 'using', 'yelp', 'find', 'devon', 'horse', 'show',\n",
    "    'urge', 'turn', 'ad', 'blocker', 'telegraph', 'barbecue', 'stop', 'crunched',\n",
    "    'porch', 'ebay', 'amazon', 'curry', 'weeknightsset', 'easy', 'dinner', 'matter', 'partner',\n",
    "    'find', 'detailed', 'description', 'apartment', 'got', 'news', 'mission', 'day', 'impersonal',\n",
    "    'get', 'tip', 'top', 'mirror', 'newsletter', 'sign', 'thank', 'subscribing',\n",
    "    'newsletter', 'invalid', 'full', 'swing', 'keen', 'get', 'hand', 'high', 'street',\n",
    "    'john', 'lewis', 'curry', 'ton', 'currently', 'available', 'actual', 'check', 'back', 'also', 'honor',\n",
    "    'writer', 'try', 'put', 'apartment', 'rent', 'via', 'go', 'rounded', 'dog', 'shelter', 'pup',\n",
    "    'dozen', 'donut', 'south', 'targeted', 'practise', 'floridado', 'love', 'florida', 'doggy',\n",
    "    'cancer', 'hide', 'caption', 'cooky', 'browser', 'sauce', 'pandemicthe',\n",
    "    'something', 'penguina', 'eagle', 'email', 'notification', 'irvinein', 'hoodline',\n",
    "    'recipe', 'perfect', 'meal', 'googlethe', 'v' \n",
    "]\n",
    "\n",
    "\n",
    "stop_words.extend(eda_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "def lemmatize_text(text):\n",
    "     return [lemmatizer.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tokens = df.tokens.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tokens = df.tokens.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tokens = df.tokens.apply(unlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tokens[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freqdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist=FreqDist(df.tokens)\n",
    "fdist.plot(15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for additional stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = df.drop_duplicates(subset=['tokens'])\n",
    "text = cloud.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(tweet for tweet in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,17))\n",
    "\n",
    "wordcloud = WordCloud(max_words=200,collocations=False, width=1000, height=700, background_color=\"black\").generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "#wordcloud.to_file('all_tweets_wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "rf_feats = df.tokens.values\n",
    "tfidfconverter = TfidfVectorizer(max_features=10000, ngram_range=(1,3))  \n",
    "\n",
    "X = tfidfconverter.fit_transform(rf_feats).toarray()\n",
    "\n",
    "tfidf_df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "frames = [df, tfidf_df]\n",
    "main_df = pd.concat(frames, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_csv('main_data/maindf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df.index).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
