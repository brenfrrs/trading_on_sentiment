{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import pytz\n",
    "from nltk.probability import FreqDist\n",
    "from custom_scripts import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading on Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will address the data cleaning steps needed in order to have a dataset suitable for modeling and analysis. We will import all of the yearly articles, convert the datetimes to Eastern Standard Time, tag the sentiment of each article, and combine each article with its historical stock price information for the day it was published. Finally we will tokenize, remove stop words, then aggregate all of the values so that we have one row of information per day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import yearly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37709, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/Volumes/pimyllifeupshare/training_data/Goldman Sachs2020.csv', index_col=0)\n",
    "df2 = pd.read_csv('/Volumes/pimyllifeupshare/training_data/Goldman Sachs2019.csv', index_col=0)\n",
    "df3 = pd.read_csv('/Volumes/pimyllifeupshare/training_data/Goldman Sachs2018.csv', index_col=0)\n",
    "df4 = pd.read_csv('/Volumes/pimyllifeupshare/training_data/Goldman Sachs2017.csv', index_col=0)\n",
    "df5 = pd.read_csv('/Volumes/pimyllifeupshare/training_data/Goldman Sachs2016.csv', index_col=0)\n",
    "df6 = pd.read_csv('/Volumes/pimyllifeupshare/training_data/Goldman Sachs2015.csv', index_col=0)\n",
    "df = pd.concat([df1,df2, df3, df4, df5, df6])\n",
    "df.dropna(subset=['fulltext'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean newlines and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 s, sys: 595 ms, total: 28.6 s\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['cleaned_text'] = df['fulltext'].apply(clean_text)\n",
    "df['cleaned_authors'] = df['author'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changing the UTC time to EST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to Datetime\n",
    "df[\"date\"]= pd.to_datetime(df[\"date\"])\n",
    "df = df.set_index('date')\n",
    "df.index = df.index.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 262 ms, sys: 5.26 ms, total: 268 ms\n",
      "Wall time: 275 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#convert DateTime index to eastern time. \n",
    "eastern = pytz.timezone('US/Eastern')\n",
    "df.index = df.index.tz_convert(eastern).tz_localize(None)\n",
    "#put into year/month/day format\n",
    "df.index = df.index.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using [tldextract](https://pypi.org/project/tldextract/) to extract company names from url's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 37709 different articles from 595 news outlets \n",
      "\n",
      "CPU times: user 365 ms, sys: 7.48 ms, total: 373 ms\n",
      "Wall time: 377 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['news_outlet'] = df['source'].apply(get_outlet)\n",
    "print('The dataset contains {} different articles from {} news outlets \\n'.format(df.shape[0],df.news_outlet.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting historical Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_we_need = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "full_date_list = []\n",
    "\n",
    "for year in years_we_need:\n",
    "    res = get_month_day_range(year)\n",
    "    full_date_list += res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_past_prices` custom function uses a list of dates and a ticker symbol to call the twelvedata.com API for all of the dates in the provided list. In our case we want historical prices over the past 5 years, because we have 5 years worth of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) 2015-01-01 to 2015-01-31\n",
      "2) 2015-02-01 to 2015-02-28\n",
      "3) 2015-03-01 to 2015-03-31\n",
      "4) 2015-04-01 to 2015-04-30\n",
      "5) 2015-05-01 to 2015-05-31\n",
      "6) 2015-06-01 to 2015-06-30\n",
      "7) 2015-07-01 to 2015-07-31\n",
      "8) 2015-08-01 to 2015-08-31\n",
      "9) 2015-09-01 to 2015-09-30\n",
      "10) 2015-10-01 to 2015-10-31\n",
      "11) 2015-11-01 to 2015-11-30\n",
      "12) 2015-12-01 to 2015-12-31\n",
      "13) 2016-01-01 to 2016-01-31\n",
      "14) 2016-02-01 to 2016-02-29\n",
      "15) 2016-03-01 to 2016-03-31\n",
      "16) 2016-04-01 to 2016-04-30\n",
      "17) 2016-05-01 to 2016-05-31\n",
      "18) 2016-06-01 to 2016-06-30\n",
      "19) 2016-07-01 to 2016-07-31\n",
      "20) 2016-08-01 to 2016-08-31\n",
      "21) 2016-09-01 to 2016-09-30\n",
      "22) 2016-10-01 to 2016-10-31\n",
      "23) 2016-11-01 to 2016-11-30\n",
      "24) 2016-12-01 to 2016-12-31\n",
      "25) 2017-01-01 to 2017-01-31\n",
      "26) 2017-02-01 to 2017-02-28\n",
      "27) 2017-03-01 to 2017-03-31\n",
      "28) 2017-04-01 to 2017-04-30\n",
      "29) 2017-05-01 to 2017-05-31\n",
      "30) 2017-06-01 to 2017-06-30\n",
      "31) 2017-07-01 to 2017-07-31\n",
      "32) 2017-08-01 to 2017-08-31\n",
      "33) 2017-09-01 to 2017-09-30\n",
      "34) 2017-10-01 to 2017-10-31\n",
      "35) 2017-11-01 to 2017-11-30\n",
      "36) 2017-12-01 to 2017-12-31\n",
      "37) 2018-01-01 to 2018-01-31\n",
      "38) 2018-02-01 to 2018-02-28\n",
      "39) 2018-03-01 to 2018-03-31\n",
      "40) 2018-04-01 to 2018-04-30\n",
      "41) 2018-05-01 to 2018-05-31\n",
      "42) 2018-06-01 to 2018-06-30\n",
      "43) 2018-07-01 to 2018-07-31\n",
      "44) 2018-08-01 to 2018-08-31\n",
      "45) 2018-09-01 to 2018-09-30\n",
      "46) 2018-10-01 to 2018-10-31\n",
      "47) 2018-11-01 to 2018-11-30\n",
      "48) 2018-12-01 to 2018-12-31\n",
      "49) 2019-01-01 to 2019-01-31\n",
      "50) 2019-02-01 to 2019-02-28\n",
      "51) 2019-03-01 to 2019-03-31\n",
      "52) 2019-04-01 to 2019-04-30\n",
      "53) 2019-05-01 to 2019-05-31\n",
      "54) 2019-06-01 to 2019-06-30\n",
      "55) 2019-07-01 to 2019-07-31\n",
      "56) 2019-08-01 to 2019-08-31\n",
      "57) 2019-09-01 to 2019-09-30\n",
      "58) 2019-10-01 to 2019-10-31\n",
      "59) 2019-11-01 to 2019-11-30\n",
      "60) 2019-12-01 to 2019-12-31\n",
      "61) 2020-01-01 to 2020-01-31\n",
      "62) 2020-02-01 to 2020-02-29\n",
      "63) 2020-03-01 to 2020-03-31\n",
      "64) 2020-04-01 to 2020-04-30\n",
      "65) 2020-05-01 to 2020-05-31\n",
      "66) 2020-06-01 to 2020-06-30\n",
      "67) 2020-07-01 to 2020-07-31\n",
      "68) 2020-08-01 to 2020-08-31\n",
      "69) 2020-09-01 to 2020-09-30\n",
      "70) 2020-10-01 to 2020-10-31\n",
      "71) 2020-11-01 to 2020-11-30\n",
      "72) 2020-12-01 to 2020-12-31\n",
      "Final shape: (1506, 5)\n"
     ]
    }
   ],
   "source": [
    "historical_prices = get_past_prices(full_date_list, 'GS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop below iterates through the historical prices and calculates the change in a stock price from one open to another. Adding a 0 if the stock decreased or there was not change, and adding a 1 if the stock increased. This is an initial tagging step, the threshold for targets can be adjusted later using the 'day_change' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_index = historical_prices.index.strftime('%Y-%m-%d').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_prices.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(columns = ['day_change', 'increase', 'date'])\n",
    "for i,stock_price in enumerate(prices_index):\n",
    "    try:\n",
    "        today = historical_prices.loc[prices_index[i]].open\n",
    "        tomorrow = historical_prices.loc[prices_index[i+1]].open\n",
    "        direction = tomorrow - today\n",
    "        if direction < 0:\n",
    "            increase = 0\n",
    "        else:\n",
    "            increase = 1\n",
    "        df_res = df_res.append({'day_change': direction, 'increase':increase, 'date':stock_price}, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_change</th>\n",
       "      <th>increase</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>1.50999</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>-1.09999</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>-0.15001</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-12-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>-5.20999</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>-0.11999</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-12-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      day_change increase        date\n",
       "1500     1.50999        1  2020-12-08\n",
       "1501    -1.09999        0  2020-12-07\n",
       "1502    -0.15001        0  2020-12-04\n",
       "1503    -5.20999        0  2020-12-03\n",
       "1504    -0.11999        0  2020-12-02"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[\"date\"]= pd.to_datetime(df_res[\"date\"])\n",
    "df_res = df_res.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.sort_index(inplace=True)\n",
    "targets = df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the historical prices with the daily change we calculated and the targets. \n",
    "targs=pd.merge(targets,historical_prices, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_change</th>\n",
       "      <th>increase</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-12-24</th>\n",
       "      <td>-5.04999</td>\n",
       "      <td>0</td>\n",
       "      <td>256.54999</td>\n",
       "      <td>257.57999</td>\n",
       "      <td>253.75000</td>\n",
       "      <td>256.16000</td>\n",
       "      <td>957649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>-1.26001</td>\n",
       "      <td>0</td>\n",
       "      <td>257.81000</td>\n",
       "      <td>262.65500</td>\n",
       "      <td>257.00000</td>\n",
       "      <td>259.59000</td>\n",
       "      <td>2732245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>-2.45001</td>\n",
       "      <td>0</td>\n",
       "      <td>260.26001</td>\n",
       "      <td>260.85999</td>\n",
       "      <td>256.50000</td>\n",
       "      <td>258.01001</td>\n",
       "      <td>1430400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>1.45001</td>\n",
       "      <td>1</td>\n",
       "      <td>258.81000</td>\n",
       "      <td>260.64999</td>\n",
       "      <td>257.82999</td>\n",
       "      <td>259.45001</td>\n",
       "      <td>1566500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            day_change increase       open       high        low      close  \\\n",
       "2020-12-24    -5.04999        0  256.54999  257.57999  253.75000  256.16000   \n",
       "2020-12-28    -1.26001        0  257.81000  262.65500  257.00000  259.59000   \n",
       "2020-12-29    -2.45001        0  260.26001  260.85999  256.50000  258.01001   \n",
       "2020-12-30     1.45001        1  258.81000  260.64999  257.82999  259.45001   \n",
       "\n",
       "             volume  \n",
       "2020-12-24   957649  \n",
       "2020-12-28  2732245  \n",
       "2020-12-29  1430400  \n",
       "2020-12-30  1566500  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the targets\n",
    "targs.to_csv('gs_targs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge targets and main data on the date\n",
    "df=pd.merge(df,targs, how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for weekends and holidays when the market is closed. Forward filling of the previous non-NA value is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill', inplace = True)\n",
    "\n",
    "#Drop the few late 2014 values where we have not price data. \n",
    "df.dropna(subset=['increase', 'open', 'high', 'low', 'close'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Sentiment for each Article with VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tag the sentiment of each article, we will use the [VADER](https://github.com/cjhutto/vaderSentiment) sentiment analyzer. The `sentiment_analyzer_scores` custom function inputs a string and output the result of the VADER sentiment prediction. Vader is primarily used for social media text; however, is effective with news articles as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7c286051eda2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n#tag the sentiment for each article using VADER. This will take a few minutes.\\ndf['sentiment'] = df['fulltext'].apply(sentiment_analyzer_scores)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/brendanferris/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/brendanferris/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brendanferris/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/brendanferris/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4043\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4045\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/brendanferris/Desktop/scripts/trading_on_sentiment/custom_scripts.py\u001b[0m in \u001b[0;36msentiment_analyzer_scores\u001b[0;34m(article)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brendanferris/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36mpolarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment_valence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentitext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_but_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brendanferris/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36msentiment_valence\u001b[0;34m(self, valence, sentitext, item, i, sentiments)\u001b[0m\n\u001b[1;32m    310\u001b[0m                         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0mvalence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     \u001b[0mvalence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_negation_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                         \u001b[0mvalence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_special_idioms_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brendanferris/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36m_negation_check\u001b[0;34m(valence, words_and_emoticons, start_i, i)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_negation_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mwords_and_emoticons_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnegated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords_and_emoticons_lower\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1 word preceding lexicon word (w/o stopwords)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brendanferris/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/vaderSentiment/vaderSentiment.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_negation_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mwords_and_emoticons_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_and_emoticons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnegated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords_and_emoticons_lower\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1 word preceding lexicon word (w/o stopwords)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#tag the sentiment for each article using VADER. This will take a few minutes.\n",
    "df['sentiment'] = df['fulltext'].apply(sentiment_analyzer_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After predicting sentiment of the article, we can create dummies of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dummies = pd.get_dummies(df['sentiment'], prefix='sent')\n",
    "df = pd.concat([df, sentiment_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['cleaned_text'].apply(toke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize/Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three custom functions: `remove_stopwords`, `lemmatize_text`, `unlist` will be used to process the word tokens we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = [remove_stopwords, lemmatize_text, unlist]\n",
    "\n",
    "for action in pre_process:\n",
    "    df.tokens = df.tokens.apply(action)\n",
    "    print('Completed: {}'.format(str(action)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure there are no duplicate articles.\n",
    "df.drop_duplicates(subset=['tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out irellevant articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"/Users/brendanferris/Desktop/scripts/trading_on_sentiment/main_data/apple/seperated_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def relevant_news(article):\n",
    "    t = re.findall(r'\\b[A-Za-z]{2,25}\\b', article.lower())\n",
    "    relevant_keywords = dict(Counter(t))\n",
    "    \n",
    "    freq_dist = pd.DataFrame(list(relevant_keywords.items()), columns=['word', 'freq'])\n",
    "    freq_dist = freq_dist.sort_values(by=['freq'], ascending=False)\n",
    "    freq_dist = freq_dist.reset_index(drop=True)\n",
    "    \n",
    "    apple_mentions = freq_dist.loc[freq_dist['word'] == 'apple']\n",
    "    stock_mentions = freq_dist.loc[(freq_dist['word'] == 'stock') | (freq_dist['word'] == 'stocks') | (freq_dist['word'] == 'market') | (freq_dist['word'] == 'wall') | (freq_dist['word'] == 'street') | (freq_dist['word'] == 'analyst')]\n",
    "    company_mentions = freq_dist.loc[(freq_dist['word'] == 'company') | (freq_dist['word'] == 'tim')]\n",
    "    product_mentions = freq_dist.loc[(freq_dist['word'] == 'iphone') | (freq_dist['word'] == 'phone') | (freq_dist['word'] == 'iphones') | (freq_dist['word'] == 'macbook') | (freq_dist['word'] == 'watch') | (freq_dist['word'] == 'ipad') | (freq_dist['word'] == 'itunes')]\n",
    "    technology_mentions = freq_dist.loc[(freq_dist['word'] == 'technology') | (freq_dist['word'] == 'tech')]\n",
    "    \n",
    "    try:\n",
    "        if int(sum(apple_mentions.freq.values)) >= 3 or int(sum(stock_mentions.freq.values)) >= 2 or int(sum(company_mentions.freq.values)) >= 1 or int(sum(product_mentions.freq.values)) > 1 or int(sum(technology_mentions.freq.values)) >= 1:\n",
    "            return \"POSSIBLE\"\n",
    "        else:\n",
    "            return \"PROBABLY NOT\"\n",
    "    except IndexError:\n",
    "        return \"PROBABLY NOT\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_relevant'] = data['fulltext'].apply(relevant_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>update</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>fulltext</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_authors</th>\n",
       "      <th>news_outlet</th>\n",
       "      <th>day_change</th>\n",
       "      <th>...</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sent_negative</th>\n",
       "      <th>sent_neutral</th>\n",
       "      <th>sent_positive</th>\n",
       "      <th>tokens</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>2015-01-05 00:00:00</td>\n",
       "      <td>http://gizmodo.com/4k-drone-footage-captures-a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>After looking at rendered images for years, se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4K Drone Footage Captures Apple's New Spaceshi...</td>\n",
       "      <td>after looking at rendered images for years see...</td>\n",
       "      <td></td>\n",
       "      <td>gizmodo</td>\n",
       "      <td>-0.7750</td>\n",
       "      <td>...</td>\n",
       "      <td>26.83750</td>\n",
       "      <td>27.3325</td>\n",
       "      <td>53204600.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>looking, rendered, image, year, seeing, real, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>PROBABLY NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>2015-01-09 10:44:15+00:00</td>\n",
       "      <td>http://www.mirror.co.uk/news/uk-news/apple-ipo...</td>\n",
       "      <td>['Pete Bainbridge', 'Image', 'Men']</td>\n",
       "      <td>Our free email newsletter sends you the bigges...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apple iPods being used as SPY CAMERAS by thiev...</td>\n",
       "      <td>our free email newsletter sends you the bigges...</td>\n",
       "      <td>pete bainbridge image men</td>\n",
       "      <td>mirror</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>...</td>\n",
       "      <td>27.17500</td>\n",
       "      <td>27.9725</td>\n",
       "      <td>59364500.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>free, sends, biggest, headline, sport, showbiz...</td>\n",
       "      <td>1</td>\n",
       "      <td>PROBABLY NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-11</th>\n",
       "      <td>2015-01-12 00:00:00</td>\n",
       "      <td>http://www.huffingtonpost.com/2015/01/12/apple...</td>\n",
       "      <td>['Senior Culture Reporter']</td>\n",
       "      <td>Actually, this character is inspired from the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here's A Bottle Of Apple Juice That Looks Some...</td>\n",
       "      <td>actually this character is inspired from the p...</td>\n",
       "      <td>senior culture reporter</td>\n",
       "      <td>huffingtonpost</td>\n",
       "      <td>-0.0175</td>\n",
       "      <td>...</td>\n",
       "      <td>27.55250</td>\n",
       "      <td>28.0025</td>\n",
       "      <td>53699500.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>actually, character, inspired, pre, modern, ja...</td>\n",
       "      <td>1</td>\n",
       "      <td>PROBABLY NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-13</th>\n",
       "      <td>2015-01-13 23:48:06+00:00</td>\n",
       "      <td>http://www.news.com.au/world/breaking-news/app...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Apple camera patent makes GoPro nervous\\n\\nAPP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apple camera patent makes GoPro nervous</td>\n",
       "      <td>apple camera patent makes gopro nervous apple ...</td>\n",
       "      <td></td>\n",
       "      <td>news</td>\n",
       "      <td>-0.5975</td>\n",
       "      <td>...</td>\n",
       "      <td>27.22750</td>\n",
       "      <td>27.5550</td>\n",
       "      <td>67091900.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>apple, camera, patent, make, gopro, nervous, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>PROBABLY NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-17</th>\n",
       "      <td>2015-01-18 10:28:01+00:00</td>\n",
       "      <td>http://www.inquisitr.com/1765290/mother-tried-...</td>\n",
       "      <td>['Addam Corré']</td>\n",
       "      <td>A 29-year-old mother of two, who reportedly tr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mother Tried To Kill Her Kids With Drugged App...</td>\n",
       "      <td>a 29 year old mother of two who reportedly tri...</td>\n",
       "      <td>addam corr</td>\n",
       "      <td>inquisitr</td>\n",
       "      <td>0.2025</td>\n",
       "      <td>...</td>\n",
       "      <td>26.30000</td>\n",
       "      <td>26.4975</td>\n",
       "      <td>78513300.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29, year, old, mother, two, reportedly, tried,...</td>\n",
       "      <td>1</td>\n",
       "      <td>PROBABLY NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-16</th>\n",
       "      <td>2020-08-16 00:00:00</td>\n",
       "      <td>https://thetakeout.com/how-magical-is-apple-ci...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Doesn’t it look magical, though? Photo : Madel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How magical is apple cider vinegar, really?</td>\n",
       "      <td>doesn t it look magical though photo madeleine...</td>\n",
       "      <td></td>\n",
       "      <td>thetakeout</td>\n",
       "      <td>1.1900</td>\n",
       "      <td>...</td>\n",
       "      <td>113.04750</td>\n",
       "      <td>114.9075</td>\n",
       "      <td>17809052.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>look, magical, though, photo, madeleine, stein...</td>\n",
       "      <td>1</td>\n",
       "      <td>PROBABLY NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-16</th>\n",
       "      <td>2020-08-18 07:18:57+10:00</td>\n",
       "      <td>https://news.yahoo.com/fortnite-maker-says-app...</td>\n",
       "      <td>[]</td>\n",
       "      <td>The Week\\n\\nNo one expected Donald Trump and h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'Fortnite' maker says Apple threatened to cut ...</td>\n",
       "      <td>the week no one expected donald trump and his ...</td>\n",
       "      <td></td>\n",
       "      <td>yahoo</td>\n",
       "      <td>1.1900</td>\n",
       "      <td>...</td>\n",
       "      <td>113.04750</td>\n",
       "      <td>114.9075</td>\n",
       "      <td>17809052.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>week, one, expected, donald, trump, supporter,...</td>\n",
       "      <td>1</td>\n",
       "      <td>PROBABLY NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-17</th>\n",
       "      <td>2020-08-18 00:00:00</td>\n",
       "      <td>https://www.wbtv.com/2020/08/18/apple-picking-...</td>\n",
       "      <td>['Steve Ohnesorge', 'Published At Pm']</td>\n",
       "      <td>Officials say there’s simply not enough of the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apple picking underway, pickers spreading out ...</td>\n",
       "      <td>officials say there s simply not enough of the...</td>\n",
       "      <td>steve ohnesorge published at pm</td>\n",
       "      <td>wbtv</td>\n",
       "      <td>-1.5700</td>\n",
       "      <td>...</td>\n",
       "      <td>113.97500</td>\n",
       "      <td>114.5825</td>\n",
       "      <td>15808241.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>official, say, simply, enough, vaccine, everyo...</td>\n",
       "      <td>1</td>\n",
       "      <td>PROBABLY NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18</th>\n",
       "      <td>2020-08-19 10:56:43+00:00</td>\n",
       "      <td>https://www.independent.co.uk/arts-entertainme...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Fiona Apple has spoken candidly about her feel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fiona Apple says ICE agents ‘must have failed ...</td>\n",
       "      <td>fiona apple has spoken candidly about her feel...</td>\n",
       "      <td></td>\n",
       "      <td>independent</td>\n",
       "      <td>1.5525</td>\n",
       "      <td>...</td>\n",
       "      <td>114.02500</td>\n",
       "      <td>115.5625</td>\n",
       "      <td>13908263.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fiona, apple, spoken, candidly, feeling, towar...</td>\n",
       "      <td>1</td>\n",
       "      <td>PROBABLY NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-19</th>\n",
       "      <td>2020-08-20 16:59:52+08:00</td>\n",
       "      <td>https://www.scmp.com/news/hong-kong/law-and-cr...</td>\n",
       "      <td>['Brian Wong', 'Updated', 'Aug']</td>\n",
       "      <td>Apple Daily founder Jimmy Lai appears at the W...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lawyers accuse rival paper of harassment as Ap...</td>\n",
       "      <td>apple daily founder jimmy lai appears at the w...</td>\n",
       "      <td>brian wong updated aug</td>\n",
       "      <td>scmp</td>\n",
       "      <td>-0.2375</td>\n",
       "      <td>...</td>\n",
       "      <td>115.61002</td>\n",
       "      <td>115.7075</td>\n",
       "      <td>17741532.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>apple, daily, founder, jimmy, lai, appears, we...</td>\n",
       "      <td>1</td>\n",
       "      <td>PROBABLY NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2115 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               update  \\\n",
       "2015-01-04        2015-01-05 00:00:00   \n",
       "2015-01-08  2015-01-09 10:44:15+00:00   \n",
       "2015-01-11        2015-01-12 00:00:00   \n",
       "2015-01-13  2015-01-13 23:48:06+00:00   \n",
       "2015-01-17  2015-01-18 10:28:01+00:00   \n",
       "...                               ...   \n",
       "2020-08-16        2020-08-16 00:00:00   \n",
       "2020-08-16  2020-08-18 07:18:57+10:00   \n",
       "2020-08-17        2020-08-18 00:00:00   \n",
       "2020-08-18  2020-08-19 10:56:43+00:00   \n",
       "2020-08-19  2020-08-20 16:59:52+08:00   \n",
       "\n",
       "                                                       source  \\\n",
       "2015-01-04  http://gizmodo.com/4k-drone-footage-captures-a...   \n",
       "2015-01-08  http://www.mirror.co.uk/news/uk-news/apple-ipo...   \n",
       "2015-01-11  http://www.huffingtonpost.com/2015/01/12/apple...   \n",
       "2015-01-13  http://www.news.com.au/world/breaking-news/app...   \n",
       "2015-01-17  http://www.inquisitr.com/1765290/mother-tried-...   \n",
       "...                                                       ...   \n",
       "2020-08-16  https://thetakeout.com/how-magical-is-apple-ci...   \n",
       "2020-08-16  https://news.yahoo.com/fortnite-maker-says-app...   \n",
       "2020-08-17  https://www.wbtv.com/2020/08/18/apple-picking-...   \n",
       "2020-08-18  https://www.independent.co.uk/arts-entertainme...   \n",
       "2020-08-19  https://www.scmp.com/news/hong-kong/law-and-cr...   \n",
       "\n",
       "                                            author  \\\n",
       "2015-01-04                                      []   \n",
       "2015-01-08     ['Pete Bainbridge', 'Image', 'Men']   \n",
       "2015-01-11             ['Senior Culture Reporter']   \n",
       "2015-01-13                                      []   \n",
       "2015-01-17                         ['Addam Corré']   \n",
       "...                                            ...   \n",
       "2020-08-16                                      []   \n",
       "2020-08-16                                      []   \n",
       "2020-08-17  ['Steve Ohnesorge', 'Published At Pm']   \n",
       "2020-08-18                                      []   \n",
       "2020-08-19        ['Brian Wong', 'Updated', 'Aug']   \n",
       "\n",
       "                                                     fulltext  summary  \\\n",
       "2015-01-04  After looking at rendered images for years, se...      NaN   \n",
       "2015-01-08  Our free email newsletter sends you the bigges...      NaN   \n",
       "2015-01-11  Actually, this character is inspired from the ...      NaN   \n",
       "2015-01-13  Apple camera patent makes GoPro nervous\\n\\nAPP...      NaN   \n",
       "2015-01-17  A 29-year-old mother of two, who reportedly tr...      NaN   \n",
       "...                                                       ...      ...   \n",
       "2020-08-16  Doesn’t it look magical, though? Photo : Madel...      NaN   \n",
       "2020-08-16  The Week\\n\\nNo one expected Donald Trump and h...      NaN   \n",
       "2020-08-17  Officials say there’s simply not enough of the...      NaN   \n",
       "2020-08-18  Fiona Apple has spoken candidly about her feel...      NaN   \n",
       "2020-08-19  Apple Daily founder Jimmy Lai appears at the W...      NaN   \n",
       "\n",
       "                                                        title  \\\n",
       "2015-01-04  4K Drone Footage Captures Apple's New Spaceshi...   \n",
       "2015-01-08  Apple iPods being used as SPY CAMERAS by thiev...   \n",
       "2015-01-11  Here's A Bottle Of Apple Juice That Looks Some...   \n",
       "2015-01-13            Apple camera patent makes GoPro nervous   \n",
       "2015-01-17  Mother Tried To Kill Her Kids With Drugged App...   \n",
       "...                                                       ...   \n",
       "2020-08-16        How magical is apple cider vinegar, really?   \n",
       "2020-08-16  'Fortnite' maker says Apple threatened to cut ...   \n",
       "2020-08-17  Apple picking underway, pickers spreading out ...   \n",
       "2020-08-18  Fiona Apple says ICE agents ‘must have failed ...   \n",
       "2020-08-19  Lawyers accuse rival paper of harassment as Ap...   \n",
       "\n",
       "                                                 cleaned_text  \\\n",
       "2015-01-04  after looking at rendered images for years see...   \n",
       "2015-01-08  our free email newsletter sends you the bigges...   \n",
       "2015-01-11  actually this character is inspired from the p...   \n",
       "2015-01-13  apple camera patent makes gopro nervous apple ...   \n",
       "2015-01-17  a 29 year old mother of two who reportedly tri...   \n",
       "...                                                       ...   \n",
       "2020-08-16  doesn t it look magical though photo madeleine...   \n",
       "2020-08-16  the week no one expected donald trump and his ...   \n",
       "2020-08-17  officials say there s simply not enough of the...   \n",
       "2020-08-18  fiona apple has spoken candidly about her feel...   \n",
       "2020-08-19  apple daily founder jimmy lai appears at the w...   \n",
       "\n",
       "                            cleaned_authors     news_outlet  day_change  ...  \\\n",
       "2015-01-04                                          gizmodo     -0.7750  ...   \n",
       "2015-01-08        pete bainbridge image men          mirror      0.8600  ...   \n",
       "2015-01-11          senior culture reporter  huffingtonpost     -0.0175  ...   \n",
       "2015-01-13                                             news     -0.5975  ...   \n",
       "2015-01-17                       addam corr       inquisitr      0.2025  ...   \n",
       "...                                     ...             ...         ...  ...   \n",
       "2020-08-16                                       thetakeout      1.1900  ...   \n",
       "2020-08-16                                            yahoo      1.1900  ...   \n",
       "2020-08-17  steve ohnesorge published at pm            wbtv     -1.5700  ...   \n",
       "2020-08-18                                      independent      1.5525  ...   \n",
       "2020-08-19           brian wong updated aug            scmp     -0.2375  ...   \n",
       "\n",
       "                  low     close      volume  sentiment  sent_negative  \\\n",
       "2015-01-04   26.83750   27.3325  53204600.0   positive              0   \n",
       "2015-01-08   27.17500   27.9725  59364500.0   negative              1   \n",
       "2015-01-11   27.55250   28.0025  53699500.0   positive              0   \n",
       "2015-01-13   27.22750   27.5550  67091900.0   negative              1   \n",
       "2015-01-17   26.30000   26.4975  78513300.0   negative              1   \n",
       "...               ...       ...         ...        ...            ...   \n",
       "2020-08-16  113.04750  114.9075  17809052.0   positive              0   \n",
       "2020-08-16  113.04750  114.9075  17809052.0   positive              0   \n",
       "2020-08-17  113.97500  114.5825  15808241.0    neutral              0   \n",
       "2020-08-18  114.02500  115.5625  13908263.0   positive              0   \n",
       "2020-08-19  115.61002  115.7075  17741532.0   negative              1   \n",
       "\n",
       "            sent_neutral sent_positive  \\\n",
       "2015-01-04             0             1   \n",
       "2015-01-08             0             0   \n",
       "2015-01-11             0             1   \n",
       "2015-01-13             0             0   \n",
       "2015-01-17             0             0   \n",
       "...                  ...           ...   \n",
       "2020-08-16             0             1   \n",
       "2020-08-16             0             1   \n",
       "2020-08-17             1             0   \n",
       "2020-08-18             0             1   \n",
       "2020-08-19             0             0   \n",
       "\n",
       "                                                       tokens  total_articles  \\\n",
       "2015-01-04  looking, rendered, image, year, seeing, real, ...               1   \n",
       "2015-01-08  free, sends, biggest, headline, sport, showbiz...               1   \n",
       "2015-01-11  actually, character, inspired, pre, modern, ja...               1   \n",
       "2015-01-13  apple, camera, patent, make, gopro, nervous, a...               1   \n",
       "2015-01-17  29, year, old, mother, two, reportedly, tried,...               1   \n",
       "...                                                       ...             ...   \n",
       "2020-08-16  look, magical, though, photo, madeleine, stein...               1   \n",
       "2020-08-16  week, one, expected, donald, trump, supporter,...               1   \n",
       "2020-08-17  official, say, simply, enough, vaccine, everyo...               1   \n",
       "2020-08-18  fiona, apple, spoken, candidly, feeling, towar...               1   \n",
       "2020-08-19  apple, daily, founder, jimmy, lai, appears, we...               1   \n",
       "\n",
       "             is_relevant  \n",
       "2015-01-04  PROBABLY NOT  \n",
       "2015-01-08  PROBABLY NOT  \n",
       "2015-01-11  PROBABLY NOT  \n",
       "2015-01-13  PROBABLY NOT  \n",
       "2015-01-17  PROBABLY NOT  \n",
       "...                  ...  \n",
       "2020-08-16  PROBABLY NOT  \n",
       "2020-08-16  PROBABLY NOT  \n",
       "2020-08-17  PROBABLY NOT  \n",
       "2020-08-18  PROBABLY NOT  \n",
       "2020-08-19  PROBABLY NOT  \n",
       "\n",
       "[2115 rows x 23 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.is_relevant == 'PROBABLY NOT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Approach (More general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate the daily news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform modeling on the aggregated article text per day. Our data is in a format that has each row as a new article, we want to aggregate all of the articles on a given day into a single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agged = df.copy()\n",
    "agged.reset_index(inplace=True)\n",
    "agged['date'] = pd.to_datetime(agged['index'])\n",
    "agged.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a column of 1's for when we aggregate all info into one column, we can add the 1's later to get the total articles per day. \n",
    "agged['total_articles'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = agged.groupby('date')['sent_negative', 'sent_positive', 'total_articles'].agg(np.sum)\n",
    "text = agged.groupby('date')['tokens'].agg(''.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agged = pd.merge(text, sentiment, how='inner', left_index=True, right_index=True)\n",
    "#Merge targets\n",
    "agged = pd.merge(agged, targs, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agged.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the cleaned dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe will be used in the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agged.to_pickle('main_data/gs/maindf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle('main_data/gs/seperated_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
