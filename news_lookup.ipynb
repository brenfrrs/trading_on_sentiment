{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import fulltext, Article\n",
    "from datetime import date, timedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selectorlib import Extractor\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapes all of the links for a company from 2015 - Present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep in main notebook for easier error handling. \n",
    "\n",
    "def fetch_articles(company):    \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d-%m-%Y_%I-%M-%S%p\")\n",
    "    \n",
    "    dates = [('#tp_2', dt_string)]\n",
    "\n",
    "    for date_button, date in dates:\n",
    "\n",
    "        ChromeOptions = webdriver.ChromeOptions()\n",
    "        ChromeOptions.add_argument('--incognito')\n",
    "        ChromeOptions.add_argument('--headless')\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install(), options=ChromeOptions)\n",
    "        time.sleep(2)\n",
    "        print('navigating to newslookup..')\n",
    "        news_look = \"https://www.newslookup.com\"\n",
    "        time.sleep(1)\n",
    "        driver.get(news_look)\n",
    "        time.sleep(3)\n",
    "\n",
    "        driver.find_element_by_css_selector('#lookup').send_keys(company)\n",
    "\n",
    "        #search button\n",
    "        driver.find_element_by_css_selector('#form-group > div > span > button').click()\n",
    "\n",
    "        #time period\n",
    "        driver.find_element_by_css_selector('#timeperiod').click()\n",
    "\n",
    "        #2020--------------------------------------------------\n",
    "        driver.find_element_by_css_selector(date_button).click()\n",
    "\n",
    "        #scroll down\n",
    "        for loaded_page in range(10000):\n",
    "            try:\n",
    "                print(f'{loaded_page}')\n",
    "                time.sleep(.90)\n",
    "                driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "                time.sleep(.90)\n",
    "                driver.find_element_by_css_selector('#more-btn').click()\n",
    "            except ElementNotInteractableException: \n",
    "                print('no more pages')\n",
    "                break\n",
    "            except (TimeoutException, ElementClickInterceptedException) as e: #extraneous errors usually fixed after retry.\n",
    "                continue\n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "\n",
    "        results_list = [] #list of all the links and associated publish dates. \n",
    "\n",
    "        raw_html = driver.page_source\n",
    "        extracted_text = Extractor.from_yaml_string(\"\"\"\n",
    "        card:\n",
    "            css: 'div#results'\n",
    "            xpath: null\n",
    "            type: Text\n",
    "            children:\n",
    "                title:\n",
    "                    css: 'a.title:nth-of-type(n+4)'\n",
    "                    xpath: null\n",
    "                    multiple: true\n",
    "                    type: Link\n",
    "                date:\n",
    "                    css: span.stime\n",
    "                    xpath: null\n",
    "                    multiple: true\n",
    "                    type: Text\n",
    "\n",
    "         \"\"\")\n",
    "        raw_data = extracted_text.extract(raw_html)\n",
    "\n",
    "        #combines the links with their appropriate publish dates (dates in UTC format at this point.)\n",
    "        results = list(zip(raw_data['card']['title'],raw_data['card']['date'][1:]))\n",
    "        for entry in results:\n",
    "            results_list.append(entry)\n",
    "\n",
    "        def getTxt(lst):\n",
    "            url, posttime = lst\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            try:\n",
    "                article.parse()\n",
    "                text = article.text\n",
    "                auth = article.authors\n",
    "                source = lst\n",
    "                summ = article.summary\n",
    "                titl = article.title\n",
    "                dic = {'time':posttime,'source':url, 'author':auth, 'fulltext':text, 'summary':summ, 'title':titl}\n",
    "\n",
    "                return dic\n",
    "            except Exception as e:\n",
    "                return None #we can remove null values from results list.\n",
    "            \n",
    "        print(\"Downloading Articles...\")    \n",
    "        pool = ThreadPool(15)\n",
    "        # open the urls in their own threads\n",
    "        # and return the results\n",
    "        raw_output = pool.map(getTxt, results_list)\n",
    "        results = [record for record in raw_output if record] #only return a list of truthy values. \n",
    "\n",
    "        # close the pool and wait for the work to finish \n",
    "        pool.close() \n",
    "        pool.join()   \n",
    "        \n",
    "        #compile and save the final dataframe.\n",
    "        final_df = pd.DataFrame(results)\n",
    "        final_df = final_df.drop_duplicates(subset='fulltext')\n",
    "        final_df.to_csv('/Volumes/pimyllifeupshare/{}_{}.csv'.format(company, date))\n",
    "        driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 88.0.4324\n",
      "[WDM] - Get LATEST driver version for 88.0.4324\n",
      "[WDM] - Driver [/Users/brendanferris/.wdm/drivers/chromedriver/mac64/88.0.4324.96/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "navigating to newslookup..\n",
      "0\n",
      "1\n",
      "2\n",
      "no more pages\n",
      "Downloading Articles...\n"
     ]
    }
   ],
   "source": [
    "fetch_articles('Goldman Sachs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
