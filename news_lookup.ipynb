{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import fulltext, Article\n",
    "from datetime import date, timedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selectorlib import Extractor\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapes all of the links for a company from 2015 - Present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep in main notebook for easier error handling. \n",
    "\n",
    "def fetch_articles(company):\n",
    "    ChromeOptions = webdriver.ChromeOptions()\n",
    "    ChromeOptions.add_argument('--incognito')\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=ChromeOptions)\n",
    "    time.sleep(2)\n",
    "    news_look = \"https://www.newslookup.com\"\n",
    "    time.sleep(1)\n",
    "    driver.get(news_look)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_css_selector('#lookup').send_keys(company)\n",
    "    \n",
    "    #search button\n",
    "    driver.find_element_by_css_selector('#form-group > div > span > button').click()\n",
    "    \n",
    "    #time period\n",
    "    driver.find_element_by_css_selector('#timeperiod').click()\n",
    "    \n",
    "    #2020--------------------------------------------------\n",
    "    driver.find_element_by_css_selector('#tp_-720').click()\n",
    "    \n",
    "    #scroll down\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_css_selector('#more-btn').click()\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    pano = []\n",
    "\n",
    "    r = driver.page_source\n",
    "    e = Extractor.from_yaml_string(\"\"\"\n",
    "    card:\n",
    "        css: 'div#results'\n",
    "        xpath: null\n",
    "        type: Text\n",
    "        children:\n",
    "            title:\n",
    "                css: 'a.title:nth-of-type(n+4)'\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Link\n",
    "            date:\n",
    "                css: span.stime\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Text\n",
    "\n",
    "     \"\"\")\n",
    "    dat = e.extract(r)\n",
    "\n",
    "    inf = list(zip(dat['card']['title'],dat['card']['date'][1:]))\n",
    "    for entry in inf:\n",
    "        pano.append(entry)\n",
    "        \n",
    "    final_df = pd.DataFrame(columns= ['update', 'date', 'source', 'author', 'fulltext', 'summary'])\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    for link, date in pano:\n",
    "        try:\n",
    "            article= Article(link)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            text = article.text\n",
    "            published = article.publish_date\n",
    "            auth = article.authors\n",
    "            source = link\n",
    "            summ = article.summary\n",
    "            titl = article.title\n",
    "            final_df = final_df.append({'update':published, 'date':date, 'source':link, 'author':auth, 'fulltext':text, 'summary':summ, 'title':titl}, ignore_index=True)\n",
    "            counter += 1\n",
    "            print(counter)\n",
    "            final_df = final_df.drop_duplicates(subset='fulltext')\n",
    "            print(final_df.shape)\n",
    "        except:\n",
    "            continue\n",
    "    final_df = final_df.drop_duplicates(subset='fulltext')       \n",
    "    final_df.to_csv('yearly_articles/{}2020.csv'.format(company))\n",
    "    \n",
    "    #2019-------------------------------------------------------\n",
    "    driver.find_element_by_tag_name('body').send_keys(Keys.HOME)\n",
    "    \n",
    "    driver.find_element_by_css_selector('#timeperiod').click()\n",
    "    \n",
    "    driver.find_element_by_css_selector('#tp_Y2019').click()\n",
    "    \n",
    "    #scroll down\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_css_selector('#more-btn').click()\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    pano = []\n",
    "\n",
    "    r = driver.page_source\n",
    "    e = Extractor.from_yaml_string(\"\"\"\n",
    "    card:\n",
    "        css: 'div#results'\n",
    "        xpath: null\n",
    "        type: Text\n",
    "        children:\n",
    "            title:\n",
    "                css: 'a.title:nth-of-type(n+4)'\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Link\n",
    "            date:\n",
    "                css: span.stime\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Text\n",
    "\n",
    "     \"\"\")\n",
    "    dat = e.extract(r)\n",
    "\n",
    "    inf = list(zip(dat['card']['title'],dat['card']['date'][1:]))\n",
    "    for entry in inf:\n",
    "        pano.append(entry)\n",
    "        \n",
    "    final_df = pd.DataFrame(columns= ['update', 'date', 'source', 'author', 'fulltext', 'summary'])\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    for link, date in pano:\n",
    "        try:\n",
    "            article= Article(link)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            text = article.text\n",
    "            published = article.publish_date\n",
    "            auth = article.authors\n",
    "            source = link\n",
    "            summ = article.summary\n",
    "            titl = article.title\n",
    "            final_df = final_df.append({'update':published, 'date':date, 'source':link, 'author':auth, 'fulltext':text, 'summary':summ, 'title':titl}, ignore_index=True)\n",
    "            counter += 1\n",
    "            print(counter)\n",
    "            final_df = final_df.drop_duplicates(subset='fulltext')\n",
    "            print(final_df.shape)\n",
    "        except:\n",
    "            continue\n",
    "    final_df = final_df.drop_duplicates(subset='fulltext')       \n",
    "    final_df.to_csv('yearly_articles/{}2019.csv'.format(company))\n",
    "    \n",
    "    #2018-------------------------------------------------------\n",
    "    driver.find_element_by_tag_name('body').send_keys(Keys.HOME)\n",
    "    \n",
    "    driver.find_element_by_css_selector('#timeperiod').click()\n",
    "    \n",
    "    driver.find_element_by_css_selector('#tp_Y2018').click()\n",
    "    \n",
    "    #scroll down\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_css_selector('#more-btn').click()\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    pano = []\n",
    "\n",
    "    r = driver.page_source\n",
    "    e = Extractor.from_yaml_string(\"\"\"\n",
    "    card:\n",
    "        css: 'div#results'\n",
    "        xpath: null\n",
    "        type: Text\n",
    "        children:\n",
    "            title:\n",
    "                css: 'a.title:nth-of-type(n+4)'\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Link\n",
    "            date:\n",
    "                css: span.stime\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Text\n",
    "\n",
    "     \"\"\")\n",
    "    dat = e.extract(r)\n",
    "\n",
    "    inf = list(zip(dat['card']['title'],dat['card']['date'][1:]))\n",
    "    for entry in inf:\n",
    "        pano.append(entry)\n",
    "        \n",
    "    final_df = pd.DataFrame(columns= ['update', 'date', 'source', 'author', 'fulltext', 'summary'])\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    for link, date in pano:\n",
    "        try:\n",
    "            article= Article(link)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            text = article.text\n",
    "            published = article.publish_date\n",
    "            auth = article.authors\n",
    "            source = link\n",
    "            summ = article.summary\n",
    "            titl = article.title\n",
    "            final_df = final_df.append({'update':published, 'date':date, 'source':link, 'author':auth, 'fulltext':text, 'summary':summ, 'title':titl}, ignore_index=True)\n",
    "            counter += 1\n",
    "            print(counter)\n",
    "            final_df = final_df.drop_duplicates(subset='fulltext')\n",
    "            print(final_df.shape)\n",
    "        except:\n",
    "            continue\n",
    "    final_df = final_df.drop_duplicates(subset='fulltext')       \n",
    "    final_df.to_csv('yearly_articles/{}2018.csv'.format(company))\n",
    "    \n",
    "    #2017-------------------------------------------------------\n",
    "    driver.find_element_by_tag_name('body').send_keys(Keys.HOME)\n",
    "    \n",
    "    driver.find_element_by_css_selector('#timeperiod').click()\n",
    "    \n",
    "    driver.find_element_by_css_selector('#tp_Y2017').click()\n",
    "    \n",
    "    #scroll down\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_css_selector('#more-btn').click()\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    pano = []\n",
    "\n",
    "    r = driver.page_source\n",
    "    e = Extractor.from_yaml_string(\"\"\"\n",
    "    card:\n",
    "        css: 'div#results'\n",
    "        xpath: null\n",
    "        type: Text\n",
    "        children:\n",
    "            title:\n",
    "                css: 'a.title:nth-of-type(n+4)'\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Link\n",
    "            date:\n",
    "                css: span.stime\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Text\n",
    "\n",
    "     \"\"\")\n",
    "    dat = e.extract(r)\n",
    "\n",
    "    inf = list(zip(dat['card']['title'],dat['card']['date'][1:]))\n",
    "    for entry in inf:\n",
    "        pano.append(entry)\n",
    "        \n",
    "    final_df = pd.DataFrame(columns= ['update', 'date', 'source', 'author', 'fulltext', 'summary'])\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    for link, date in pano:\n",
    "        try:\n",
    "            article= Article(link)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            text = article.text\n",
    "            published = article.publish_date\n",
    "            auth = article.authors\n",
    "            source = link\n",
    "            summ = article.summary\n",
    "            titl = article.title\n",
    "            final_df = final_df.append({'update':published, 'date':date, 'source':link, 'author':auth, 'fulltext':text, 'summary':summ, 'title':titl}, ignore_index=True)\n",
    "            counter += 1\n",
    "            print(counter)\n",
    "            final_df = final_df.drop_duplicates(subset='fulltext')\n",
    "            print(final_df.shape)\n",
    "        except:\n",
    "            continue\n",
    "    final_df = final_df.drop_duplicates(subset='fulltext')       \n",
    "    final_df.to_csv('yearly_articles/{}2017.csv'.format(company))\n",
    "    \n",
    "    #2016-------------------------------------------------------\n",
    "    driver.find_element_by_tag_name('body').send_keys(Keys.HOME)\n",
    "    \n",
    "    driver.find_element_by_css_selector('#timeperiod').click()\n",
    "    \n",
    "    driver.find_element_by_css_selector('#tp_Y2016').click()\n",
    "    \n",
    "    #scroll down\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_css_selector('#more-btn').click()\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    pano = []\n",
    "\n",
    "    r = driver.page_source\n",
    "    e = Extractor.from_yaml_string(\"\"\"\n",
    "    card:\n",
    "        css: 'div#results'\n",
    "        xpath: null\n",
    "        type: Text\n",
    "        children:\n",
    "            title:\n",
    "                css: 'a.title:nth-of-type(n+4)'\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Link\n",
    "            date:\n",
    "                css: span.stime\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Text\n",
    "\n",
    "     \"\"\")\n",
    "    dat = e.extract(r)\n",
    "\n",
    "    inf = list(zip(dat['card']['title'],dat['card']['date'][1:]))\n",
    "    for entry in inf:\n",
    "        pano.append(entry)\n",
    "        \n",
    "    final_df = pd.DataFrame(columns= ['update', 'date', 'source', 'author', 'fulltext', 'summary'])\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    for link, date in pano:\n",
    "        try:\n",
    "            article= Article(link)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            text = article.text\n",
    "            published = article.publish_date\n",
    "            auth = article.authors\n",
    "            source = link\n",
    "            summ = article.summary\n",
    "            titl = article.title\n",
    "            final_df = final_df.append({'update':published, 'date':date, 'source':link, 'author':auth, 'fulltext':text, 'summary':summ, 'title':titl}, ignore_index=True)\n",
    "            counter += 1\n",
    "            print(counter)\n",
    "            final_df = final_df.drop_duplicates(subset='fulltext')\n",
    "            print(final_df.shape)\n",
    "        except:\n",
    "            continue\n",
    "    final_df = final_df.drop_duplicates(subset='fulltext')       \n",
    "    final_df.to_csv('yearly_articles/{}2016.csv'.format(company))\n",
    "    \n",
    "    #2015-------------------------------------------------------\n",
    "    driver.find_element_by_tag_name('body').send_keys(Keys.HOME)\n",
    "    \n",
    "    driver.find_element_by_css_selector('#timeperiod').click()\n",
    "    \n",
    "    driver.find_element_by_css_selector('#tp_Y2015').click()\n",
    "    \n",
    "    #scroll down\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "            time.sleep(1.5)\n",
    "            driver.find_element_by_css_selector('#more-btn').click()\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    pano = []\n",
    "\n",
    "    r = driver.page_source\n",
    "    e = Extractor.from_yaml_string(\"\"\"\n",
    "    card:\n",
    "        css: 'div#results'\n",
    "        xpath: null\n",
    "        type: Text\n",
    "        children:\n",
    "            title:\n",
    "                css: 'a.title:nth-of-type(n+4)'\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Link\n",
    "            date:\n",
    "                css: span.stime\n",
    "                xpath: null\n",
    "                multiple: true\n",
    "                type: Text\n",
    "\n",
    "     \"\"\")\n",
    "    dat = e.extract(r)\n",
    "\n",
    "    inf = list(zip(dat['card']['title'],dat['card']['date'][1:]))\n",
    "    for entry in inf:\n",
    "        pano.append(entry)\n",
    "        \n",
    "    final_df = pd.DataFrame(columns= ['update', 'date', 'source', 'author', 'fulltext', 'summary'])\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    for link, date in pano:\n",
    "        try:\n",
    "            article= Article(link)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            text = article.text\n",
    "            published = article.publish_date\n",
    "            auth = article.authors\n",
    "            source = link\n",
    "            summ = article.summary\n",
    "            titl = article.title\n",
    "            final_df = final_df.append({'update':published, 'date':date, 'source':link, 'author':auth, 'fulltext':text, 'summary':summ, 'title':titl}, ignore_index=True)\n",
    "            counter += 1\n",
    "            print(counter)\n",
    "            final_df = final_df.drop_duplicates(subset='fulltext')\n",
    "            print(final_df.shape)\n",
    "        except:\n",
    "            continue\n",
    "    final_df = final_df.drop_duplicates(subset='fulltext')       \n",
    "    final_df.to_csv('yearly_articles/{}2015.csv'.format(company))\n",
    "    \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fetch_articles('Nike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
